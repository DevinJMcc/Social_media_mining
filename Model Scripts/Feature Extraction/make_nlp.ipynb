{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "make_nlp.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxbB2D_9kcK7",
        "outputId": "33d6ccf0-4984-401a-fa96-76a1390370c9"
      },
      "source": [
        "import pandas as pd\n",
        "import sys\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import re\n",
        "import string\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "'''\n",
        "Some processing on the text files. we are going through and doing some of the\n",
        "basic NLP steps. We start by tokenizing the words, then removing  stop words,\n",
        "lemmatize the words, and stem the words.\n",
        "'''\n",
        "def tokenDocs(inTxt):\n",
        "\n",
        "    # Intialize the stop word calculator, lemmatizer, and stemmer\n",
        "    nltk_stopwords = nltk.corpus.stopwords.words('english')\n",
        "    wordnet_lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "    snowball_stemmer = nltk.stem.SnowballStemmer('english')\n",
        "\n",
        "\n",
        "    inTxt = re.sub('@[\\w]+','',inTxt) # remove username\n",
        "    inTxt = re.sub(r'http\\S+', '', inTxt) # remove website\n",
        "    inTxt = re.sub('<f0>\\S+','',inTxt) # remove utf\n",
        "    inTxt = re.sub('<\\S+>','',inTxt) # remove utf\n",
        "\n",
        "    # remove punc.\n",
        "    # for c in string.punctuation:\n",
        "    #   inTxt = inTxt.replace(c,\"\")\n",
        "    \n",
        "    # Tokenize the documents\n",
        "    tokens = nltk.word_tokenize(inTxt)\n",
        "\n",
        "    # Convert all things to lower case, make sure things are numeric, remove stopwords,\n",
        "    # lemmatize and stem things\n",
        "    tokens = [token.lower() for token in tokens]\n",
        "    tokens = [token for token in tokens if token.isalpha()]\n",
        "    tokens = [token for token in tokens if token not in nltk_stopwords]\n",
        "    tokens = [snowball_stemmer.stem(token) for token in tokens]\n",
        "\n",
        "\n",
        "    return ' '.join([str(item) for item in tokens]) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krTBgzqBmPu3"
      },
      "source": [
        "df = pd.read_csv('vax_sideeffects-2.csv', engine='python')\n",
        "df['text'] = df['text'].apply(tokenDocs)\n",
        "\n",
        "new_df = df.drop(columns=['screen_name','screen_name','Unnamed: 0','Unnamed: 0.1'])\n",
        "# new_df = new_df.dropna()\n",
        "\n",
        "new_df.to_csv('vax-sideeffects-FINAL.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}