# -*- coding: utf-8 -*-
"""model_search.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dSebm7kSPfA4Z0eEvhphnUZDOISoQfBr
"""

''' scikit-learn is a Python library for Machine Learning.
    We will make use of their implementation of a decision tree classifier
    https://scikit-learn.org/stable/modules/tree.html,
    a function that helps us split data into training and test, and
    a function that helps us evaluate the accuracy of our model'''
from sklearn import tree
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.model_selection import StratifiedKFold
from sklearn import svm
import pandas as pd
import numpy as np
import sys
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import AdaBoostClassifier
from xgboost.sklearn import XGBClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier

'''
Run a normal decision tree classifier. Out put the top 5 classifiers and the the features importance for all the 
features being used
'''
def runDecisionTreeClassifier(x_train,y_train,x_test,y_test):

    # Here we instantiate the decision tree classifier
    clf = tree.DecisionTreeClassifier()

    parameter_grid = {'criterion': ['gini', 'entropy'],
                  'splitter': ['best', 'random'],
                  'min_samples_split': [3, 4], # reg
                  'max_depth': [1, 2, 3, 4], # regularize
                  'min_samples_leaf': [2,3], # reg
                  'max_features': list(range(1,len(x_train.columns)+1))
                      }
    grid_search = GridSearchCV(clf, param_grid=parameter_grid, return_train_score=True,  n_jobs=20, cv=StratifiedKFold(n_splits=10))
    grid_search.fit(x_train,y_train)
    print('Best score: {}'.format(grid_search.best_score_), file=sys.stderr)
    print('Best parameters: {}'.format(grid_search.best_params_), file=sys.stderr)

    # refit and train the model to the best features and training data
    clf = grid_search.best_estimator_

    # now, make the predictions using our classifier
    dt_predictions = clf.predict(x_test)

    # now we have to computer the classification accuracy
    # think about what two variables we have to compare
    dt_score = accuracy_score(y_test, dt_predictions)
    print("decision tree classification accuracy on test data is " + str(dt_score), file=sys.stderr)

    # let's compare it to a naive algorithm that
    # simply selects the order code at random
    random_sample = np.random.choice([0,1], len(y_test), replace=True)
    r_score = accuracy_score(y_test, random_sample)
    print("random algorithm classification accuracy on test data is " + str(r_score), file=sys.stderr)

    print("feature importances " + str(clf.feature_importances_), file=sys.stderr)

    # Print the feature ranking
    print("Feature ranking:", file=sys.stderr)
    importanceDict = {'names':[],'imp':[]}
    for name, importance in zip(x_train.columns, clf.feature_importances_):
        importanceDict['names'] += [name]
        importanceDict['imp'] += [importance]
    fRank = pd.DataFrame.from_dict(importanceDict)
    fRank = fRank.sort_values(by='imp',ascending=False)
    i = 0
    for index, row in fRank.iterrows():
        print("%d. %s %f"%(i, row['names'],row['imp']), file=sys.stderr)
        i += 1

    cv_results = pd.DataFrame(grid_search.cv_results_)[['rank_test_score', 'params','mean_test_score','mean_train_score']]
    sorted_results = cv_results.sort_values(by='rank_test_score').head(5)

    print("\nTop 5 best Parameters: ", file=sys.stderr)
    for index, row in sorted_results.iterrows():
        print("%d. %s train: %s test: %s" % (row['rank_test_score'], str(row['params']), str(row['mean_train_score']), str(row['mean_test_score'])), file=sys.stderr)


    etc_predictions = clf.predict(x_test)
    dt_score = accuracy_score(y_test, etc_predictions)
    print("accuracy score on test data: " + str(dt_score), file=sys.stderr)
    train_score = accuracy_score(y_train, clf.predict(x_train))
    print("accuracy score on training data: " + str( train_score ), file=sys.stderr)

    return (train_score,dt_score)

'''
Function to run the gradient boosting classifier on the data given. Out put the top 5 classifiers and the the features 
importance for all the features being used
'''
def runGradientBoostingClassifier(x_train,y_train,x_test,y_test):

    clf = GradientBoostingClassifier()

    parameter_grid = {
        "learning_rate": np.linspace(0.001, .01, 3),# best below .1
        "min_samples_split": np.linspace(.3, .5, 2),  # original bot .1 to .5 with 12 - reg
        "min_samples_leaf": np.linspace(.3, .5, 2), # reg
        'max_depth': [1, 2, 3], # reg
        "subsample":[0.5, 0.6, 0.7], # reg
        "n_estimators":[100, 200, 300 ] # reg - larger slower to overfit
    }

    grid_search = GridSearchCV(clf, n_jobs=40, return_train_score=True, param_grid=parameter_grid, cv=StratifiedKFold(n_splits=10))
    grid_search.fit(x_train,y_train)
    print('Best score: {}'.format(grid_search.best_score_), file=sys.stderr)
    print('Best parameters: {}'.format(grid_search.best_params_), file=sys.stderr)

    clf = grid_search.best_estimator_

    gbc_predictions = clf.predict(x_test)
    dt_score = accuracy_score(y_test, gbc_predictions)
    print("accuracy score on test data: " +str(dt_score), file=sys.stderr)
    print("feature importances " + str(clf.feature_importances_), file=sys.stderr)

    # Print the feature ranking
    print("Feature ranking:", file=sys.stderr)
    importanceDict = {'names':[],'imp':[]}
    for name, importance in zip(x_train.columns, clf.feature_importances_):
        importanceDict['names'] += [name]
        importanceDict['imp'] += [importance]
    fRank = pd.DataFrame.from_dict(importanceDict)
    fRank = fRank.sort_values(by='imp', ascending=False)
    i = 0
    for index, row in fRank.iterrows():
        print("%d. %s %f"%(i, row['names'],row['imp']),file=sys.stderr)
        i += 1

    cv_results = pd.DataFrame(grid_search.cv_results_)[['rank_test_score', 'params','mean_test_score','mean_train_score']]
    sorted_results = cv_results.sort_values(by='rank_test_score').head(5)

    print("\nTop 5 best Parameters: ", file=sys.stderr)
    for index, row in sorted_results.iterrows():
        print("%d. %s train: %s test: %s" % (row['rank_test_score'], str(row['params']), str(row['mean_train_score']), str(row['mean_test_score'])), file=sys.stderr)



    etc_predictions = clf.predict(x_test)
    dt_score =accuracy_score(y_test, etc_predictions)
    print("accuracy score on test data: " + str(dt_score), file=sys.stderr)
    train_score = accuracy_score(y_train, clf.predict(x_train))
    print("accuracy score on training data: " + str( train_score ), file=sys.stderr)

    return (train_score,dt_score)

'''
Function to run the adaboost classifier on the data given. Out put the top 5 classifiers and the the features 
importance for all the features being used
'''
def runAdaBoostClassifier(x_train,y_train,x_test,y_test):

    clf = AdaBoostClassifier()

    parameter_grid = {
    "base_estimator": [tree.DecisionTreeClassifier(splitter='random',max_depth=1)], # lower depth and min sample split
    "learning_rate": np.linspace(0.0001, .001, 10),
    "n_estimators":[100,300,500]
    }

    grid_search = GridSearchCV(clf, n_jobs=20,  return_train_score=True, param_grid=parameter_grid, cv=StratifiedKFold(n_splits=10))
    grid_search.fit(x_train,y_train)
    print('Best score: {}'.format(grid_search.best_score_), file=sys.stderr)
    print('Best parameters: {}'.format(grid_search.best_params_), file=sys.stderr)

    clf = grid_search.best_estimator_

    print("feature importances " + str(clf.feature_importances_), file=sys.stderr)

    # Print the feature ranking
    print("Feature ranking:", file=sys.stderr)
    importanceDict = {'names':[],'imp':[]}
    for name, importance in zip(x_train.columns, clf.feature_importances_):
        importanceDict['names'] += [name]
        importanceDict['imp'] += [importance]
    fRank = pd.DataFrame.from_dict(importanceDict)
    fRank = fRank.sort_values(by='imp',ascending=False)
    i = 0
    for index, row in fRank.iterrows():
        print("%d. %s %f"%(i, row['names'],row['imp']),file=sys.stderr)
        i += 1

    cv_results = pd.DataFrame(grid_search.cv_results_)[['rank_test_score', 'params','mean_test_score','mean_train_score']]
    sorted_results = cv_results.sort_values(by='rank_test_score').head(5)

    print("\nTop 5 best Parameters: ", file=sys.stderr)
    for index, row in sorted_results.iterrows():
        print("%d. %s train: %s test: %s" % (row['rank_test_score'], str(row['params']), str(row['mean_train_score']), str(row['mean_test_score'])), file=sys.stderr)


    etc_predictions = clf.predict(x_test)
    dt_score =accuracy_score(y_test, etc_predictions)
    print("accuracy score on test data: " + str(dt_score), file=sys.stderr)
    train_score = accuracy_score(y_train, clf.predict(x_train))
    print("accuracy score on training data: " + str( train_score ), file=sys.stderr)
    return (train_score,dt_score)

'''
Function to run the extra trees classifier on the data given. Out put the top 5 classifiers and the the features 
importance for all the features being used
'''
def runExtraTreesClassifier(x_train,y_train,x_test,y_test):

    clf = ExtraTreesClassifier()

    parameter_grid = {
        "criterion": ["gini",  "entropy"],
        "n_estimators":[100,150,200,250,300,400,500],
        "max_depth":[2,3,4], # reg
        "min_samples_split": np.linspace(.3, .5, 5), # original bot .1 to .5 with 12
        "min_samples_leaf": np.linspace(.3, .5, 5),
        "max_features": ["log2", "sqrt"]
    }

    grid_search = GridSearchCV(clf, n_jobs=20,  return_train_score=True, param_grid=parameter_grid, cv=StratifiedKFold(n_splits=10))
    grid_search.fit(x_train,y_train)
    print('Best score: {}'.format(grid_search.best_score_), file=sys.stderr)
    print('Best parameters: {}'.format(grid_search.best_params_), file=sys.stderr)

    clf = grid_search.best_estimator_

    importances = clf.feature_importances_
    print(importances, file=sys.stderr)
    # Print the feature ranking
    print("Feature ranking:", file=sys.stderr)
    importanceDict = {'names':[],'imp':[]}
    for name, importance in zip(x_train.columns, clf.feature_importances_):
        importanceDict['names'] += [name]
        importanceDict['imp'] += [importance]
    fRank = pd.DataFrame.from_dict(importanceDict)
    fRank = fRank.sort_values(by='imp',ascending=False)
    i = 0
    for index, row in fRank.iterrows():
        print("%d. %s %f"%(i, row['names'],row['imp']),file=sys.stderr)
        i += 1

    cv_results = pd.DataFrame(grid_search.cv_results_)[['rank_test_score', 'params','mean_test_score','mean_train_score']]
    sorted_results = cv_results.sort_values(by='rank_test_score').head(5)

    print("\nTop 5 best Parameters: ", file=sys.stderr)
    for index, row in sorted_results.iterrows():
        print("%d. %s train: %s test: %s" % (row['rank_test_score'], str(row['params']), str(row['mean_train_score']), str(row['mean_test_score'])), file=sys.stderr)


    etc_predictions = clf.predict(x_test)
    dt_score =accuracy_score(y_test, etc_predictions)
    print("accuracy score on test data: " + str(dt_score), file=sys.stderr)
    train_score = accuracy_score(y_train, clf.predict(x_train))
    print("accuracy score on training data: " + str( train_score ), file=sys.stderr)

    return (train_score,dt_score)

def runSupportVec(x_train,y_train,x_test,y_test):

    clf = svm.SVC()
    parameter_grid = {'C': np.linspace(0.001,.1,100),
                        'gamma': [1, 0.1, 0.001, 0.0001],
                        'kernel': ['rbf']
                      }

    grid_search = GridSearchCV(clf, n_jobs=40, return_train_score=True, param_grid=parameter_grid, cv=StratifiedKFold(n_splits=10))
    grid_search.fit(x_train,y_train)
    print('Best score: {}'.format(grid_search.best_score_), file=sys.stderr)
    print('Best parameters: {}'.format(grid_search.best_params_), file=sys.stderr)

    clf = grid_search.best_estimator_

    cv_results = pd.DataFrame(grid_search.cv_results_)[['rank_test_score', 'params','mean_test_score','mean_train_score']]
    sorted_results = cv_results.sort_values(by='rank_test_score').head(5)

    print("\nTop 5 best Parameters: ", file=sys.stderr)
    for index, row in sorted_results.iterrows():
        print("%d. %s train: %s test: %s" % (row['rank_test_score'], str(row['params']), str(row['mean_train_score']), str(row['mean_test_score'])), file=sys.stderr)

    etc_predictions = clf.predict(x_test)
    dt_score = accuracy_score(y_test, etc_predictions)
    print("accuracy score on test data: " + str(dt_score), file=sys.stderr)
    train_score = accuracy_score(y_train, clf.predict(x_train))
    print("accuracy score on training data: " + str(train_score), file=sys.stderr)
    
    return (train_score, dt_score)

def runXGBoost(x_train,y_train,x_test,y_test):
    
    # parameter_grid = {
    #     "max_depth": [3,5,8], # recommeneded tune
    #     'gamma': [i / 10.0 for i in range(2, 4)],
    #     "subsample":  np.linspace(.5, .7, 2), # best between .5 and .9
    #     'colsample_bytree': np.linspace(.5, .7, 2), # best between .5 and .9
    #     'reg_alpha': [0.01, 0.05],
    #     'reg_lambda': [0.01, 0.05], # for over fitting
    #     # 'objective': ['binary:logistic', 'multi:softmax'], # most common 2
    #     "n_estimators": [100,500,1000],
	#     "learning_rate": np.linspace(.01, .3, 2), # usually between .05 and .3
    #     'nthread': [10]
    # }

    parameter_grid = {
        'reg_lambda': np.linspace(27, 28 , 2),  # for over fitting
        'reg_alpha': np.linspace(27, 28, 2),
        "learning_rate": np.linspace(.0001, .001, 3), # usually between .05 and .3
        "max_depth": [2],
        "num_boosting_rounds": [1000],
        'nthread': [10]
    }

    clf = XGBClassifier()
    grid_search = GridSearchCV(clf, n_jobs=40, return_train_score=True, param_grid=parameter_grid, cv=StratifiedKFold(n_splits=10))
    grid_search.fit(x_train,y_train)

    print('Best score: {}'.format(grid_search.best_score_), file=sys.stderr)
    print('Best parameters: {}'.format(grid_search.best_params_), file=sys.stderr)

    clf = grid_search.best_estimator_

    importances = clf.feature_importances_
    print(importances, file=sys.stderr)

    # Print the feature ranking
    print("Feature ranking:", file=sys.stderr)
    importanceDict = {'names': [], 'imp': []}
    for name, importance in zip(x_train.columns, clf.feature_importances_):
        importanceDict['names'] += [name]
        importanceDict['imp'] += [importance]
    fRank = pd.DataFrame.from_dict(importanceDict)
    fRank = fRank.sort_values(by='imp', ascending=False)
    i = 0
    for index, row in fRank.iterrows():
        print("%d. %s %f" % (i, row['names'], row['imp']), file=sys.stderr)
        i += 1

    cv_results = pd.DataFrame(grid_search.cv_results_)[['rank_test_score', 'params','mean_test_score','mean_train_score']]
    sorted_results = cv_results.sort_values(by='rank_test_score').head(5)

    print("\nTop 5 best Parameters: ", file=sys.stderr)
    for index, row in sorted_results.iterrows():
        print("%d. %s train: %s test: %s" % (row['rank_test_score'], str(row['params']), str(row['mean_train_score']), str(row['mean_test_score'])), file=sys.stderr)


    etc_predictions = clf.predict(x_test)
    dt_score = accuracy_score(y_test, etc_predictions)
    print("accuracy score on test data: " + str(dt_score), file=sys.stderr)
    train_score = accuracy_score(y_train, clf.predict(x_train))
    print("accuracy score on training data: " + str(train_score), file=sys.stderr)

    return (train_score, dt_score)

def random_trees(x_train,y_train,x_test,y_test):
    
    clf = RandomForestClassifier()
    parameter_grid = {"n_estimators": [100, 200, 250, 300, 400, 500],
                  "criterion": ["gini",  "entropy"],
                  "max_depth":[2,3,4,5],
                  "max_features": ["log2", "sqrt"],
                  "min_samples_split": np.linspace(0.3, 0.5, 10),
                  "min_samples_leaf": np.linspace(0.3, 0.5, 10),
                  'n_jobs': [10]
    }

    grid_search = GridSearchCV(clf, n_jobs=20,  return_train_score=True, param_grid=parameter_grid, cv=StratifiedKFold(n_splits=10))
    grid_search.fit(x_train,y_train)
    print('Best score: {}'.format(grid_search.best_score_), file=sys.stderr)
    print('Best parameters: {}'.format(grid_search.best_params_), file=sys.stderr)

    clf = grid_search.best_estimator_

    importances = clf.feature_importances_
    print(importances, file=sys.stderr)
    # Print the feature ranking
    print("Feature ranking:", file=sys.stderr)
    importanceDict = {'names': [], 'imp': []}
    for name, importance in zip(x_train.columns, clf.feature_importances_):
        importanceDict['names'] += [name]
        importanceDict['imp'] += [importance]
    fRank = pd.DataFrame.from_dict(importanceDict)
    fRank = fRank.sort_values(by='imp', ascending=False)
    i = 0
    for index, row in fRank.iterrows():
        print("%d. %s %f" % (i, row['names'], row['imp']), file=sys.stderr)
        i += 1

    cv_results = pd.DataFrame(grid_search.cv_results_)[['rank_test_score', 'params','mean_test_score','mean_train_score']]
    sorted_results = cv_results.sort_values(by='rank_test_score').head(5)

    print("\nTop 5 best Parameters: ", file=sys.stderr)
    for index, row in sorted_results.iterrows():
        print("%d. %s train: %s test: %s" % (row['rank_test_score'], str(row['params']), str(row['mean_train_score']), str(row['mean_test_score'])), file=sys.stderr)


    etc_predictions = clf.predict(x_test)
    dt_score = accuracy_score(y_test, etc_predictions)
    print("accuracy score on test data: " + str(dt_score), file=sys.stderr)
    train_score = accuracy_score(y_train, clf.predict(x_train))
    print("accuracy score on training data: " + str(train_score), file=sys.stderr)

    return (train_score, dt_score)

def nueral_net(x,y,X_test,y_test):
    clf = MLPClassifier()
    layers = [(int((2/3)*x.shape[1])), (int((1/2)*(x.shape[1]))), ( int((2/3)*x.shape[1]) ,int((1/2)*(x.shape[1]))),(10,8,5,2)]   
    parameter_grid = {'hidden_layer_sizes': layers,
                      'activation': ['identity', 'logistic', 'tanh', 'relu'],
                      'solver': ['lbfgs', 'sgd', 'adam'],
                      'learning_rate': ['constant', 'invscaling', 'adaptive'],
                      'learning_rate_init': [.001, 0.01, 0.025, 0.05, .075, .1],
                      'n_iter_no_change': [10, 50, 100, 150, 200, 250],
                      #'max_iter': [100, 200, 300, 400, 500],
                      'alpha': np.linspace(.0001, .05, 5)
                      }
    grid_search = GridSearchCV(clf, param_grid=parameter_grid, cv=10, n_jobs=50)
    grid_search.fit(x, y)

    clf = grid_search.best_estimator_
    clf = clf.fit(x, y)

    y_train_pred = clf.predict(x)

    print('Best score: {}'.format(accuracy_score(y, y_train_pred)))
    print('Best parameters: {}'.format(grid_search.best_params_))

    y_predictions = clf.predict(X_test)
    dt_score = accuracy_score(y_test, y_predictions)
    print("MLP accuracy on test data is " + str(dt_score))
    return grid_search.best_params_, accuracy_score(y_test, y_predictions)

if __name__ == '__main__':
# Command line arguments need to run the code
# data = file
# alg = classifier to run
# fs = features to use
  data = sys.argv[1]
  #data = '/content/word_embedings-Wiki-tfidf.csv'
  alg = int(sys.argv[2])

  # load in the data
  tweet_data = pd.read_csv(data)

  cols = list(tweet_data.columns)

  # one-hot encoding for categorical variables
  tweet_data_onehot = pd.get_dummies(tweet_data,
                columns=['sentiment','V/L'], 
                dummy_na=True)
  tweet_data_onehot = tweet_data_onehot.drop(columns=['text'])
  # split into training and test data and stratify along predicting column
  train, test = train_test_split(tweet_data_onehot, test_size=0.3,
                                stratify=tweet_data_onehot['V/L_L'])


  # handle missing data
  train = train.dropna()
  test = test.dropna()


  # data to train on
  x_train = train.drop(['V/L_L', 'V/L_V', 'V/L_nan'],axis=1)
  # and the population as the outcome (what we want to predict)
  y_train = train['V/L_L']

  # this is the test data, we do not train using this data
  x_test = test.drop(['V/L_L', 'V/L_V', 'V/L_nan'],axis=1)
  y_test = test['V/L_L']


#for alg in range(7):
  if alg == 0:
      print("decision tree classifier for " + data, file=sys.stderr)
      (accuracy_train, accuracy_test) = runDecisionTreeClassifier(x_train, y_train, x_test, y_test)
      print("\t".join(['decision tree', data, str(accuracy_train), str(accuracy_test)]))
      print("\n", file=sys.stderr)
  elif alg == 1:
      print("gradient boosting classifier for " + data, file=sys.stderr)
      (accuracy_train, accuracy_test) = runGradientBoostingClassifier(x_train, y_train, x_test, y_test)
      print("\t".join(['gradient boosting', data, str(accuracy_train), str(accuracy_test)]))
      print("\n", file=sys.stderr)
  elif alg == 2:
      print("extra trees classifier for " + data, file=sys.stderr)
      (accuracy_train, accuracy_test) = runExtraTreesClassifier(x_train, y_train, x_test, y_test)
      print("\t".join(['extra trees', data, str(accuracy_train), str(accuracy_test)]))
      print("\n", file=sys.stderr)
  elif alg == 3:
      print("adaboost classifier for " + data, file=sys.stderr)
      (accuracy_train, accuracy_test) = runAdaBoostClassifier(x_train, y_train, x_test, y_test)
      print("\t".join(['adaboost', data, str(accuracy_train), str(accuracy_test)]))
      print("\n", file=sys.stderr)
  elif alg == 4:
      print("SVM for " + data, file=sys.stderr)
      (accuracy_train, accuracy_test) = runSupportVec(x_train, y_train, x_test, y_test)
      print("\t".join(['svm', data, str(accuracy_train), str(accuracy_test)]))
      print("\n", file=sys.stderr)
  elif alg == 5:
      print("XGB for " + data, file=sys.stderr)
      (accuracy_train, accuracy_test) = runXGBoost(x_train, y_train, x_test, y_test)
      print("\t".join(['xgb', data, str(accuracy_train), str(accuracy_test)]))
      print("\n", file=sys.stderr)
  elif alg == 6:
      print("random forest for " + data, file=sys.stderr)
      (accuracy_train, accuracy_test) = random_trees(x_train, y_train, x_test, y_test)
      print("\t".join(['random forest', data, str(accuracy_train), str(accuracy_test)]))
      print("\n", file=sys.stderr)
  elif alg == 7:
      print("MLP for " + data, file=sys.stderr)
      (accuracy_train, accuracy_test) = nueral_net(x_train, y_train, x_test, y_test)
      print("\t".join(['MLP', data, str(accuracy_train), str(accuracy_test)]))
      print("\n", file=sys.stderr)
